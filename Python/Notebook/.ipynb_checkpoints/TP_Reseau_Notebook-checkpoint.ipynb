{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travaux pratiques sur les réseaux d'eau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Données et fonctions du problème"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On charge dans l'environnement :\n",
    "1. les fonctions de Python qui seront utilisées,\n",
    "2. les données du problème,\n",
    "3. les fonctions utilitaires du problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Probleme_R import *\n",
    "from Structures_N import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualg import Visualg\n",
    "from Verification import Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  On charge les prototypes d'algorithmes d'optimisation :\n",
    "\n",
    "* **Gradient_F** : algorithme de gradient à pas fixe,\n",
    "* **Newton_F** : algorithme de Newton à pas fixe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_F(Oracle, x0, alpha):\n",
    "    \n",
    "    from numpy.linalg import norm\n",
    "    from time import process_time\n",
    "    \n",
    "    ##### Initialisation des variables\n",
    "    \n",
    "    iter_max = 10000\n",
    "    gradient_step = alpha\n",
    "    threshold = 0.000001\n",
    "    \n",
    "    gradient_norm_list = []\n",
    "    gradient_step_list = []\n",
    "    critere_list = []\n",
    "\n",
    "    time_start = process_time()\n",
    "    \n",
    "    x = x0\n",
    "\n",
    "    ##### Boucle sur les iterations\n",
    "\n",
    "    for k in range(iter_max):\n",
    "        \n",
    "        # Valeur du critere et du gradient\n",
    "        critere, gradient = Oracle(x)\n",
    "\n",
    "        # Test de convergence\n",
    "        gradient_norm = norm(gradient)\n",
    "        if gradient_norm <= threshold:\n",
    "            break\n",
    "\n",
    "        # Direction de descente\n",
    "        direction = -gradient\n",
    "        \n",
    "        # Mise a jour des variables\n",
    "        x = x + (gradient_step*direction)\n",
    "        \n",
    "        # Evolution du gradient, du pas, et du critere\n",
    "        gradient_norm_list.append(gradient_norm)\n",
    "        gradient_step_list.append(gradient_step)\n",
    "        critere_list.append(critere)\n",
    "    \n",
    "    ##### Resultats de l'optimisation\n",
    "\n",
    "    critere_opt = critere\n",
    "    gradient_opt = gradient\n",
    "    x_opt = x\n",
    "    time_cpu = process_time() - time_start\n",
    "    \n",
    "    print()\n",
    "    print('Iteration :', k)\n",
    "    print('Temps CPU :', time_cpu)\n",
    "    print('Critere optimal :', critere_opt)\n",
    "    print('Norme du gradient :', norm(gradient_opt))\n",
    "    \n",
    "    # Visualisation de la convergence\n",
    "    Visualg(gradient_norm_list, gradient_step_list, critere_list)\n",
    "    \n",
    "    return critere_opt, gradient_opt, x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_F(Oracle, x0):\n",
    "    \n",
    "    from numpy.linalg import norm\n",
    "    from numpy.linalg import solve\n",
    "    from time import process_time\n",
    "    \n",
    "    ##### Initialisation des variables    \n",
    "    \n",
    "    iter_max = 100\n",
    "    gradient_step = 1\n",
    "    threshold = 0.000001\n",
    "    \n",
    "    gradient_norm_list = []\n",
    "    gradient_step_list = []\n",
    "    critere_list = []\n",
    "\n",
    "    time_start = process_time()\n",
    "    \n",
    "    x = x0\n",
    "\n",
    "    ##### Boucle sur les iterations\n",
    "    \n",
    "    for k in range(iter_max):\n",
    "        \n",
    "        # Valeur du critere et du gradient\n",
    "        critere, gradient, hessien = Oracle(x)\n",
    "\n",
    "        # Test de convergence\n",
    "        gradient_norm = norm(gradient)\n",
    "        if gradient_norm <= threshold:\n",
    "            break\n",
    "        \n",
    "        # Direction de descente\n",
    "        direction = - solve(hessien, gradient)\n",
    "        \n",
    "        # Mise a jour des variables\n",
    "        x = x + (gradient_step*direction)\n",
    "        \n",
    "        # Evolution du gradient, du pas, et du critere\n",
    "        gradient_norm_list.append(gradient_norm)\n",
    "        gradient_step_list.append(gradient_step)\n",
    "        critere_list.append(critere)\n",
    "   \n",
    "    ##### Resultats de l'optimisation\n",
    "    \n",
    "    critere_opt = critere\n",
    "    gradient_opt = gradient\n",
    "    x_opt = x\n",
    "    time_cpu = process_time() - time_start\n",
    "\n",
    "    print()\n",
    "    print('Iteration :', k)\n",
    "    print('Temps CPU :', time_cpu)\n",
    "    print('Critere optimal :', critere_opt)\n",
    "    print('Norme du gradient :', norm(gradient_opt))\n",
    "    \n",
    "    # Visualisation de la convergence\n",
    "    Visualg(gradient_norm_list, gradient_step_list, critere_list)\n",
    "    \n",
    "    return critere_opt, gradient_opt, x_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Résolution du problème primal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HydrauliqueP import HydrauliqueP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.a On écrit les oracles pour le problème primal d'optimisation :\n",
    "\n",
    "* **OraclePG** : calcul de la fonction et du gradient,\n",
    "* **OraclePH** : calcul de la fonction du gradient et du Hessien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OraclePG(qc):\n",
    "    # valeur du critère évalué au point qc\n",
    "    term1 = q0 + B.dot(qc)\n",
    "    term2 = np.multiply(np.multiply(r, term1), np.abs(term1))\n",
    "    term3 = 1/3 * np.transpose(term2).dot(term1)\n",
    "    term4 = Ar.dot(term1)\n",
    "    term5 = np.transpose(term4).dot(pr)\n",
    "    critere = term3 + term5\n",
    "\n",
    "    # vecteur des dérivées du critère par rapport à qc\n",
    "    gradient = np.transpose(B).dot(term2) + np.transpose(B).dot(np.transpose(Ar)).dot(pr)\n",
    "    \n",
    "    return critere, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OraclePH(qc):\n",
    "    # valeur du critère évalué au point qc\n",
    "    term1 = q0 + B.dot(qc)\n",
    "    term2 = np.multiply(np.multiply(r, term1), np.abs(term1))\n",
    "    term3 = 1/3 * np.transpose(term2).dot(term1)\n",
    "    term4 = Ar.dot(term1)\n",
    "    term5 = np.transpose(term4).dot(pr)\n",
    "    critere = term3 + term5\n",
    "\n",
    "    # vecteur des dérivées du critère par rapport à qc\n",
    "    gradient = np.transpose(B).dot(term2) + np.transpose(B).dot(np.transpose(Ar)).dot(pr)\n",
    "\n",
    "    # matrice des dérivées secondes du critère par rapport à qc\n",
    "    term6 = 2 * np.multiply(r, np.abs(term1))\n",
    "    term7 = np.diag(term6.flatten())\n",
    "    term8 = np.transpose(B).dot(term7)\n",
    "    hessien = term8.dot(B)\n",
    "    \n",
    "    return critere, gradient, hessien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oracle(qc):\n",
    "    # valeur du critère évalué au point qc\n",
    "    term1 = q0 + B.dot(qc)\n",
    "    term2 = np.multiply(np.multiply(r, term1), np.abs(term1))\n",
    "    term3 = 1/3 * np.transpose(term2).dot(term1)\n",
    "    term4 = Ar.dot(term1)\n",
    "    term5 = np.transpose(term4).dot(pr)\n",
    "    critere = term3 + term5\n",
    "\n",
    "    # vecteur des dérivées du critère par rapport à qc\n",
    "    gradient = np.transpose(B).dot(term2) + np.transpose(B).dot(np.transpose(Ar)).dot(pr)\n",
    "\n",
    "    # matrice des dérivées secondes du critère par rapport à qc\n",
    "    term6 = 2 * np.multiply(r, np.abs(term1))\n",
    "    term7 = np.diag(term6.flatten())\n",
    "    term8 = np.transpose(B).dot(term7)\n",
    "    hessien = term8.dot(B)\n",
    "    return critere, gradient, hessien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On teste la validité des oracles en résolvant le problème :\n",
    "\n",
    "* **OraclePG** avec la méthode du gradient à pas fixe,\n",
    "* **OraclePH** avec la méthode de Newton à pas fixe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.1 * random.normal(size=n-md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME PRIMAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DU GRADIENT A PAS FIXE\")\n",
    "copt, gopt, xopt = Gradient_F(OraclePG, x0, 0.0005)\n",
    "qopt, zopt, fopt, popt = HydrauliqueP(xopt, m, mr, md, r, pr, fd, Ar, AdI, B, q0)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.1 * random.normal(size=n-md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME PRIMAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DE NEWTON A PAS FIXE\")\n",
    "copt, gopt, xopt = Newton_F(OraclePH, x0)\n",
    "qopt, zopt, fopt, popt = HydrauliqueP(xopt, m, mr, md, r, pr, fd, Ar, AdI, B, q0)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.b On écrit l'algorithme de recherche linéaire (conditions de Wolfe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wolfe(alpha, x, D, Oracle):\n",
    "    \n",
    "    from numpy import dot\n",
    "    from numpy import inf\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    ##### Coefficients de la recherche lineaire\n",
    "    \n",
    "    omega_1 = 0.1\n",
    "    omega_2 = 0.9\n",
    "    \n",
    "    alpha_min = 0\n",
    "    alpha_max = inf\n",
    "    \n",
    "    ok = 0\n",
    "    dltx = 0.00000001\n",
    "\n",
    "    ##### Algorithme de Fletcher-Lemarechal\n",
    "    \n",
    "    # Appel de l'oracle au point initial\n",
    "    argout = Oracle(x)\n",
    "    critere = argout[0]\n",
    "    gradient = argout[1]\n",
    "    \n",
    "    # Initialisation de l'algorithme\n",
    "    alpha_n = alpha\n",
    "    xn = x\n",
    "    \n",
    "    # Boucle de calcul du pas\n",
    "    # xn represente le point pour la valeur courante du pas,\n",
    "    # xp represente le point pour la valeur precedente du pas.\n",
    "    while ok == 0:\n",
    "        \n",
    "        # Point precedent pour tester l'indistinguabilite\n",
    "        xp = xn\n",
    "        \n",
    "        # Point actuel\n",
    "        xn = x + alpha_n*D\n",
    "        \n",
    "        # Calcul des conditions de Wolfe\n",
    "        #\n",
    "        # ---> A COMPLETER\n",
    "        # ---> A COMPLETER\n",
    "        \n",
    "        # Test des conditions de Wolfe\n",
    "        #\n",
    "        # - Si les deux conditions de Wolfe sont verifiees,\n",
    "        #   poser ok = 1 : on sort alors de la boucle while\n",
    "        # - Sinon, modifier la valeur de alphan et reboucler\n",
    "        #\n",
    "        # ---> A COMPLETER\n",
    "        # ---> A COMPLETER\n",
    "        \n",
    "        # Test d'indistinguabilite\n",
    "        if norm(xn - xp) < dltx:\n",
    "            ok = 2\n",
    "\n",
    "    return alpha_n, ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.c On écrit l'algorithme du gradient à pas variable et on le teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_V(Oracle, x0, alpha):\n",
    "    \n",
    "    from numpy.linalg import norm\n",
    "    from time import process_time\n",
    "    \n",
    "    ##### Initialisation des variables\n",
    "\n",
    "    iter_max = 1000\n",
    "    default_gradient_step = alpha\n",
    "    threshold = 0.000001\n",
    "    \n",
    "    gradient_norm_list = []\n",
    "    gradient_step_list = []\n",
    "    critere_list = []\n",
    "    \n",
    "    time_start = process_time()\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    ##### Boucle sur les iterations\n",
    "\n",
    "    for k in range(iter_max):\n",
    "        \n",
    "        # ---> A COMPLETER\n",
    "        # ---> A COMPLETER\n",
    "    \n",
    "    ##### Resultats de l'optimisation\n",
    "    critere_opt = critere\n",
    "    gradient_opt = gradient\n",
    "    x_opt = x\n",
    "    time_cpu = process_time() - time_start\n",
    "\n",
    "    print()\n",
    "    print('Iteration :', k)\n",
    "    print('Temps CPU :', time_cpu)\n",
    "    print('Critere optimal :', critere_opt)\n",
    "    print('Norme du gradient :', norm(gradient_opt))\n",
    "    \n",
    "    # Visualisation de la convergence\n",
    "    Visualg(gradient_norm_list, gradient_step_list, critere_list)\n",
    "    \n",
    "    return critere_opt, gradient_opt, x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = 0.1 * random.normal(size=n-md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME PRIMAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DU GRADIENT A PAS VARIABLE\")\n",
    "copt, gopt, xopt = Gradient_V(OraclePG, x0, 1)\n",
    "qopt, zopt, fopt, popt = HydrauliqueP(xopt, m, mr, md, r, pr, fd, Ar, AdI, B, q0)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.d On écrit l'algorithme de gradient conjugué (Polak-Ribière) et on le teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolakRibiere(Oracle, x0, alpha):\n",
    "    \n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    from time import process_time\n",
    "    \n",
    "    ##### Initialisation des variables\n",
    "\n",
    "    iter_max = 1000\n",
    "    default_gradient_step = alpha\n",
    "    threshold = 0.000001\n",
    "    \n",
    "    gradient_norm_list = []\n",
    "    gradient_step_list = []\n",
    "    critere_list = []\n",
    "    \n",
    "    time_start = process_time()\n",
    "\n",
    "    x = x0\n",
    "    \n",
    "    ##### Boucle sur les iterations\n",
    "    \n",
    "    for k in range(iter_max):\n",
    "        \n",
    "        # ---> A COMPLETER\n",
    "        # ---> A COMPLETER\n",
    "        #\n",
    "        # Attention a l'initialisation de l'algorithme !\n",
    "    \n",
    "    # Resultat de l'optimisation\n",
    "    critere_opt = critere\n",
    "    gradient_opt = gradient\n",
    "    x_opt = x\n",
    "    time_cpu = process_time() - time_start\n",
    "\n",
    "    print()\n",
    "    print('Iteration :', k)\n",
    "    print('Temps CPU :', time_cpu)\n",
    "    print('Critere optimal :', critere_opt)\n",
    "    print('Norme du gradient :', norm(gradient_opt))\n",
    "        \n",
    "    # Visualisation de la convergence\n",
    "    Visualg(gradient_norm_list, gradient_step_list, critere_list)\n",
    "    \n",
    "    return critere_opt, gradient_opt, x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.1 * random.normal(size=n-md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME PRIMAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DU GRADIENT CONJUGUE\")\n",
    "copt, gopt, xopt = PolakRibiere(OraclePG, x0, 1)\n",
    "qopt, zopt, fopt, popt = HydrauliqueP(xopt, m, mr, md, r, pr, fd, Ar, AdI, B, q0)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.e On écrit l'algorithme de quasi-Newton (BFGS) et on le teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(Oracle, x0):\n",
    "    \n",
    "    from numpy import dot\n",
    "    from numpy import eye\n",
    "    from numpy import outer\n",
    "    from numpy import vdot\n",
    "    from numpy.linalg import norm\n",
    "    from time import process_time\n",
    "    \n",
    "    ##### Initialisation des variables\n",
    "\n",
    "    iter_max = 250\n",
    "    default_gradient_step = 1\n",
    "    threshold = 0.000001\n",
    "    \n",
    "    gradient_norm_list = []\n",
    "    gradient_step_list = []\n",
    "    critere_list = []\n",
    "    \n",
    "    time_start = process_time()\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    ##### Boucle sur les iterations\n",
    "    \n",
    "    for k in range(iter_max):\n",
    "        \n",
    "        # ---> A COMPLETER\n",
    "        # ---> A COMPLETER\n",
    "        #\n",
    "        # Attention a l'initialisation de l'algorithme !\n",
    "    \n",
    "    # Resultat de l'optimisation\n",
    "    critere_opt = critere\n",
    "    gradient_opt = gradient\n",
    "    x_opt = x\n",
    "    time_cpu = process_time() - time_start\n",
    "\n",
    "    print()\n",
    "    print('Iteration :', k)\n",
    "    print('Temps CPU :', time_cpu)\n",
    "    print('Critere optimal :', critere_opt)\n",
    "    print('Norme du gradient :', norm(gradient_opt))\n",
    "        \n",
    "    # Visualisation de la convergence\n",
    "    Visualg(gradient_norm_list, gradient_step_list, critere_list)\n",
    "    \n",
    "    return critere_opt, gradient_opt, x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.1 * random.normal(size=n-md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME PRIMAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DE QUASI-NEWTON\")\n",
    "copt, gopt, xopt = BFGS(OraclePG, x0)\n",
    "qopt, zopt, fopt, popt = HydrauliqueP(xopt, m, mr, md, r, pr, fd, Ar, AdI, B, q0)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.f On écrit l'algorithme de Newton à pas variable et on le teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_V(Oracle, x0):\n",
    "    \n",
    "    from numpy.linalg import norm\n",
    "    from numpy.linalg import solve\n",
    "    from time import process_time\n",
    "    \n",
    "    ##### Initialisation des variables  \n",
    "    \n",
    "    iter_max = 100\n",
    "    default_gradient_step = 1\n",
    "    threshold = 0.000001\n",
    "    \n",
    "    gradient_norm_list = []\n",
    "    gradient_step_list = []\n",
    "    critere_list = []\n",
    "\n",
    "    time_start = process_time()\n",
    "    \n",
    "    x = x0\n",
    "\n",
    "    ##### Boucle sur les iterations\n",
    "    \n",
    "    for k in range(iter_max):\n",
    "        \n",
    "        # ---> A COMPLETER\n",
    "        # ---> A COMPLETER\n",
    "           \n",
    "    ##### Resultats de l'optimisation\n",
    "    \n",
    "    critere_opt = critere\n",
    "    gradient_opt = gradient\n",
    "    x_opt = x\n",
    "    time_cpu = process_time() - time_start\n",
    "\n",
    "    print()\n",
    "    print('Iteration :', k)\n",
    "    print('Temps CPU :', time_cpu)\n",
    "    print('Critere optimal :', critere_opt)\n",
    "    print('Norme du gradient :', norm(gradient_opt))\n",
    "    \n",
    "    # Visualisation de la convergence\n",
    "    Visualg(gradient_norm_list, gradient_step_list, critere_list)\n",
    "    \n",
    "    return critere_opt, gradient_opt, x_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.1 * random.normal(size=n-md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME PRIMAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DE NEWTON A PAS VARIABLE\")\n",
    "copt, gopt, xopt = Newton_V(OraclePH, x0)\n",
    "qopt, zopt, fopt, popt = HydrauliqueP(xopt, m, mr, md, r, pr, fd, Ar, AdI, B, q0)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Résolution du problème dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HydrauliqueD import HydrauliqueD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.a On écrit les oracles pour le problème dual d'optimisation :\n",
    "\n",
    "* **OracleDG** : calcul de la fonction et du gradient,\n",
    "* **OracleDH** : calcul de la fonction du gradient et du Hessien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OracleDG(pd):\n",
    "    \n",
    "    # ---> A COMPLETER\n",
    "    # ---> A COMPLETER\n",
    "    \n",
    "    return critere, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OracleDH(pd):\n",
    "    \n",
    "    # ---> A COMPLETER\n",
    "    # ---> A COMPLETER\n",
    "    \n",
    "    return critere, gradient, hessien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.b On exécute tous les algorithmes d'optimisation déjà écrits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 100 + random.normal(size=md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME DUAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DU GRADIENT A PAS VARIABLE\")\n",
    "copt, gopt, xopt = Gradient_V(OracleDG, x0, 5)\n",
    "qopt, zopt, fopt, popt = HydrauliqueD(xopt, m, mr, r, pr, fd, A, Ar)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 100 + random.normal(size=md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME DUAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DU GRADIENT CONJUGUE\")\n",
    "copt, gopt, xopt = PolakRibiere(OracleDG, x0, 5)\n",
    "qopt, zopt, fopt, popt = HydrauliqueD(xopt, m, mr, r, pr, fd, A, Ar)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 100 + random.normal(size=md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME DUAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DE QUASI-NEWTON\")\n",
    "copt, gopt, xopt = BFGS(OracleDG, x0)\n",
    "qopt, zopt, fopt, popt = HydrauliqueD(xopt, m, mr, r, pr, fd, A, Ar)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 100 + random.normal(size=md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME DUAL\")\n",
    "print()\n",
    "print(\"ALGORITHME DE NEWTON A PAS VARIABLE\")\n",
    "copt, gopt, xopt = Newton_V(OracleDH, x0)\n",
    "qopt, zopt, fopt, popt = HydrauliqueD(xopt, m, mr, r, pr, fd, A, Ar)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Résolution de grands réseaux (facultatif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On charge dans l'environnement les données du problème\n",
    "\n",
    "Ces données correspondent à un réseau de taille paramétrable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Probleme_P import *\n",
    "from Structures_S import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.a On écrit des oracles primal et dual avec Hessien creux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OraclePHS(qc):\n",
    "    \n",
    "    # ---> A COMPLETER\n",
    "    # ---> A COMPLETER\n",
    "    \n",
    "    return critere, gradient, hessien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OracleDHS(pd):\n",
    "    \n",
    "    # ---> A COMPLETER\n",
    "    # ---> A COMPLETER\n",
    "    \n",
    "    return critere, gradient, hessien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.b On écrit un algorithme de Newton exploitant le creux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_S(Oracle, x0):\n",
    "    \n",
    "    from numpy.linalg import norm\n",
    "    from scipy.sparse.linalg import spsolve\n",
    "    from time import process_time\n",
    "    \n",
    "    ##### Initialisation des variables  \n",
    "    \n",
    "    iter_max = 100\n",
    "    default_gradient_step = 1\n",
    "    threshold = 0.000001\n",
    "    \n",
    "    gradient_norm_list = []\n",
    "    gradient_step_list = []\n",
    "    critere_list = []\n",
    "\n",
    "    time_start = process_time()\n",
    "    \n",
    "    x = x0\n",
    "\n",
    "    ##### Boucle sur les iterations\n",
    "    \n",
    "    for k in range(iter_max):\n",
    "        \n",
    "        # ---> A COMPLETER\n",
    "        # ---> A COMPLETER\n",
    "           \n",
    "    ##### Resultats de l'optimisation\n",
    "    \n",
    "    critere_opt = critere\n",
    "    gradient_opt = gradient\n",
    "    x_opt = x\n",
    "    time_cpu = process_time() - time_start\n",
    "\n",
    "    print()\n",
    "    print('Iteration :', k)\n",
    "    print('Temps CPU :', time_cpu)\n",
    "    print('Critere optimal :', critere_opt)\n",
    "    print('Norme du gradient :', norm(gradient_opt))\n",
    "    \n",
    "    # Visualisation de la convergence\n",
    "    Visualg(gradient_norm_list, gradient_step_list, critere_list)\n",
    "    \n",
    "    return critere_opt, gradient_opt, x_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.c On résoud le problème dans le primal et dans le dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.1 * random.normal(size=n-md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME PRIMAL DE GRANDE TAILLE\")\n",
    "print()\n",
    "print(\"ALGORITHME DE NEWTON A PAS VARIABLE\")\n",
    "copt, gopt, xopt = Newton_S(OraclePHS, x0)\n",
    "qopt, zopt, fopt, popt = HydrauliqueP(xopt, m, mr, md, r, pr, fd, Ar, AdI, B, q0)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 100 + random.normal(size=md)\n",
    "print()\n",
    "print(\"MINIMISATION DU PROBLEME DUAL DE GRANDE TAILLE\")\n",
    "print()\n",
    "print(\"ALGORITHME DE NEWTON A PAS VARIABLE\")\n",
    "copt, gopt, xopt = Newton_S(OracleDHS, x0)\n",
    "qopt, zopt, fopt, popt = HydrauliqueD(xopt, m, mr, r, pr, fd, A, Ar)\n",
    "Verification(A, qopt, zopt, fopt, popt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
